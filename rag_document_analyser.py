# -*- coding: utf-8 -*-
"""Rag Document Analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MiLxgLdOJO1g73vloNzDCTHx8U3QPr8s
"""

!pip install streamlit
!pip install langchain
!pip install langchain-huggingface
!pip install langchain-community
!pip install langchain-chroma
!pip install pypdfium2
!pip install sentence-transformers
!pip install tiktoken
!pip install protobuf

import streamlit as st
import os
import tempfile
# NEW IMPORTS for free models
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.document_loaders import PyPDFium2Loader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
# Import for the modern RAG chain
from langchain_core.runnables import RunnablePassthrough

# --- Helper Functions (Caching) ---

# We use @st.cache_resource to cache the expensive operations
# like loading, chunking, and embedding. This way, it only
# runs once per uploaded file.

@st.cache_resource(show_spinner="Loading and Chunking PDF...")
def load_and_chunk_pdf(temp_file_path):
    """Loads a PDF and splits it into chunks. (From Step 1)"""
    loader = PyPDFium2Loader(temp_file_path)
    docs = loader.load()

    # Using the 1000/200 chunking from your RAG project
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(docs)
    return chunks

@st.cache_resource(show_spinner="Embedding Document...")
def create_vector_store(_chunks):
    """Creates a Chroma vector store from document chunks. (From Project 1)"""

    # This is the free, local embedding model (replaces OpenAIEmbeddings)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # Create the in-memory vector store
    vector_store = Chroma.from_documents(
        documents=_chunks,
        embedding=embeddings
    )
    return vector_store

@st.cache_resource(show_spinner="Loading Llama 3 Model...")
def get_llm(hf_token):
    """Initializes the Llama 3 LLM from Hugging Face. (The 'Free' LLM)"""
    # Set the token in the environment
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token

    # Initialize the Llama 3 model (replaces ChatOpenAI)
    llm = HuggingFaceEndpoint(
        repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
        task="text-generation",
        max_new_tokens=512,
        temperature=0.1,
        repetition_penalty=1.03,
    )
    return llm

@st.cache_resource
def get_rag_chain(_retriever, _llm):
    """Creates the RAG chain using LCEL. (From Project 1)"""

    # This is the exact prompt from your notebook, just formatted as a template
    template = """Use the following pieces of context to answer the user's question.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    ----------------
    CONTEXT: {context}

    QUESTION: {question}

    ANSWER:"""

    prompt = PromptTemplate.from_template(template)

    # This is the modern LCEL (LangChain Expression Language) chain
    # It automatically handles retrieving documents, formatting the prompt,
    # and calling the LLM.
    rag_chain = (
        {"context": _retriever | (lambda docs: "\n".join([d.page_content for d in docs])),
         "question": RunnablePassthrough()}
        | prompt
        | _llm
        | StrOutputParser()
    )

    return rag_chain

@st.cache_resource
def get_summary_chain(_llm):
    """Creates the Map-Reduce summarization chain. (From Project 2)"""

    # This is the 'chunks_prompt' from your notebook
    chunks_prompt = """
    Please summarize the below text:
    text:'{text}'
    summary:
    """
    map_prompt_template = PromptTemplate(input_variables=['text'], template=chunks_prompt)

    # This is the 'final_combine_prompt' from your notebook
    final_combine_prompt = """
    Provide a final summary of the entire text with important points.
    Add a Generic Title.
    Start the precise summary with an introduction and provide Key Points Using Bullet Points.
    text: '{text}'
    summary:
    """
    final_combine_prompt_template = PromptTemplate(input_variables=['text'], template=final_combine_prompt)

    summary_chain = load_summarize_chain(
        llm=_llm,
        chain_type='map_reduce',
        map_prompt=map_prompt_template,
        combine_prompt=final_combine_prompt_template
    )
    return summary_chain

# --- Streamlit App UI ---

st.set_page_config(page_title="PDF Analyzer", layout="wide")
st.title("ðŸ“„ Document Analyzer: Chat & Summarize (with Llama 3)")
st.markdown("This app combines RAG Q&A and Summarization using free, open-source models, based on your notebook.")

# --- 1. Sidebar for Configuration ---
with st.sidebar:
    st.header("Configuration")
    hf_api_token = st.text_input("Enter your Hugging Face API Token:", type="password")
    st.markdown("Get a free 'read' token [here](https://huggingface.co/settings/tokens).")

    st.header("Upload Document")
    uploaded_file = st.file_uploader("Upload your PDF document", type="pdf")
    st.markdown("---")
    st.markdown("Built by following the notebook logic, combining Project 1 (Q&A) and Project 2 (Summarization).")

# --- 2. Main App Logic (Combining Steps) ---

# We proceed only if we have both the token and a file
if hf_api_token and uploaded_file:
    try:
        # --- 3. Process the PDF (Step 1) ---
        # Save uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # Use cached functions to process the PDF *once*
        chunks = load_and_chunk_pdf(tmp_file_path)
        vector_store = create_vector_store(chunks)
        llm = get_llm(hf_api_token)

        # --- 4. Get Components for Both Projects (Steps 2 & 3) ---
        # Get retriever and build the chains
        retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        rag_chain = get_rag_chain(retriever, llm)
        summary_chain = get_summary_chain(llm)

        # Clean up the temporary file
        os.unlink(tmp_file_path)

        st.success(f"PDF '{uploaded_file.name}' processed successfully!")

        # --- 5. Create UI Tabs (The Combined App) ---
        tab1, tab2 = st.tabs(["Chatbot (Q&A)", "Summarization"])

        # --- TAB 1: Chatbot (Q&A) - Project 1 ---
        with tab1:
            st.subheader("Chat with Your Document")

            # Initialize chat history
            if "messages" not in st.session_state:
                st.session_state.messages = []

            # Display chat messages from history on app rerun
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            # Accept user input
            if prompt := st.chat_input("Ask a question about your document"):
                # Add user message to chat history
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    # Display user message
                    st.markdown(prompt)

                # Get AI response
                with st.chat_message("assistant"):
                    with st.spinner("Thinking..."):
                        # The new RAG chain is much cleaner to call.
                        response = rag_chain.invoke(prompt)

                        st.markdown(response)
                        # Add AI response to chat history
                        st.session_state.messages.append({"role": "assistant", "content": response})

        # --- TAB 2: Summarization - Project 2 ---
        with tab2:
            st.subheader("Get a Detailed Summary of the Document")

            if st.button("Generate Detailed Summary"):
                with st.spinner("Summarizing... This may take a moment for large documents."):
                    # Your correct fix is here
                    summary_result = summary_chain.invoke({"input_documents": chunks})
                    st.markdown(summary_result["output_text"])

    except Exception as e:
        st.error(f"An error occurred: {e}")
        st.error("Please check your Hugging Face API token or the PDF file.")

# --- 6. Initial State Messages ---
elif uploaded_file and not hf_api_token:
    st.warning("Please enter your Hugging Face API Token in the sidebar to process the document.")
elif not uploaded_file and hf_api_token:
    st.info("Please upload a PDF document to begin.")
else:
    st.info("Please upload a PDF and enter your Hugging Face API token in the sidebar to start.")