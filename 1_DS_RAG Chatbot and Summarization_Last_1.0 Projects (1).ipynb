{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nmzwD9leeUHoVYNc4qmEii-h6R79vJ5o","timestamp":1725286634532}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["___\n","\n","<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n","\n","___"],"metadata":{"id":"Jp0pcBxsVAMy"}},{"cell_type":"markdown","source":["# WELCOME\n","\n","This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n","\n","Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."],"metadata":{"id":"DxOaSxtJWV1G"}},{"cell_type":"markdown","source":["## Project 1: Building a Chatbot with a PDF Document (RAG)\n","\n","In this project, you will develop a chatbot using a provided PDF document from web page. You will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n","\n","### **Project Steps:**\n","\n","- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n","\n","- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n","\n","- **3.ChromaDB Setup:**\n","  - Save ChromaDB to your Google Drive.\n","\n","  - Retrieve ChromaDB from your Drive to begin using it in your project.\n","\n","  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n","\n","- **4.Embedding Vectors Creation:**\n","  - Convert the chunked document into embedding vectors. You can use either GPT or Gemini embedding models for this purpose.\n","\n","  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n","\n","- **5.Chatbot Development:**\n","  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n","\n","  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n","\n"],"metadata":{"id":"MaCz7nhxKI9R"}},{"cell_type":"markdown","source":["### Install Libraries"],"metadata":{"id":"_eoQWi-uN0dx"}},{"cell_type":"code","source":["!pip install -qU langchain-google-community"],"metadata":{"id":"PCbI4MuNanVu","executionInfo":{"status":"ok","timestamp":1725217936608,"user_tz":-180,"elapsed":23361,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"477b394f-3eb9-41b7-9eb0-d154a8da3b42","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m966.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU langchain-community"],"metadata":{"id":"qOaahY-AancA","executionInfo":{"status":"ok","timestamp":1725217950832,"user_tz":-180,"elapsed":4790,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -qU langchain-openai"],"metadata":{"id":"t2ctzZG9M_fE","executionInfo":{"status":"ok","timestamp":1725217964723,"user_tz":-180,"elapsed":13896,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"db95e70c-6eba-4b4c-eec0-b7fbf8ef159f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/365.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU langchain-chroma"],"metadata":{"id":"PxjcwQHuNBZB","executionInfo":{"status":"ok","timestamp":1725218000297,"user_tz":-180,"elapsed":35580,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"88c0a82b-d35e-40ac-e78b-5fa60251e77c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -qU pypdfium2"],"metadata":{"id":"XLTw3hXLNDmA","executionInfo":{"status":"ok","timestamp":1725218008489,"user_tz":-180,"elapsed":8217,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"989743dc-bf6a-4ab0-d057-59a28fd0d819","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Access Google Drive"],"metadata":{"id":"FuLllnCl2yfe"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"uQR06EhDapPP","executionInfo":{"status":"ok","timestamp":1725280917204,"user_tz":-180,"elapsed":21002,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"b1b417c2-7e02-4c29-b035-a66a7c086a8b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Entering Your OpenAI or Google Gemini API Key."],"metadata":{"id":"0uR9bJp_0MyF"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"2jwo1SQ2asnZ","executionInfo":{"status":"ok","timestamp":1725280972187,"user_tz":-180,"elapsed":2175,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"90rF1aM1astv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading PDF Document"],"metadata":{"id":"OV9rG-0PN8p0"}},{"cell_type":"code","source":["# create a pdf reader function\n","from langchain_community.document_loaders import PyPDFium2Loader\n","\n","def read_doc(directory):\n","    file_loader=PyPDFium2Loader(directory)\n","    pdf_documents=file_loader.load() # PyPDFium2Loader reads page by page\n","    return pdf_documents"],"metadata":{"id":"5H6eQyYyauxP","executionInfo":{"status":"ok","timestamp":1725218088982,"user_tz":-180,"elapsed":1546,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["pdf=read_doc('/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf')\n","len(pdf)"],"metadata":{"id":"n_kXJZ5Taupv","executionInfo":{"status":"ok","timestamp":1725218091305,"user_tz":-180,"elapsed":915,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"10c210c1-68ec-43e0-b682-fcd96da44ae4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n","  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"]},{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Document Splitter"],"metadata":{"id":"WLQ1j_JrOF57"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","\n","\n","def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n","    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n","                                                 chunk_overlap=chunk_overlap)\n","    pdf=text_splitter.split_documents(docs)\n","    return pdf\n"],"metadata":{"id":"HHQlclU9awwa","executionInfo":{"status":"ok","timestamp":1725218098682,"user_tz":-180,"elapsed":331,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["pdf_doc=chunk_data(docs=pdf)\n","len(pdf_doc)"],"metadata":{"id":"NaQV6XRwawpf","executionInfo":{"status":"ok","timestamp":1725218101613,"user_tz":-180,"elapsed":335,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"2c26bd75-59e8-45fb-f118-02839ae30669","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["84"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### 1. Creating A Embedding Model\n","### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n","### 3. Storing of The Embedding Vectors to Vectorstore\n","### 4. Save the Vectorstore to Your Drive"],"metadata":{"id":"4ENim_5MOT9O"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\",\n","                            dimensions=3072) #dimensions=256, 1024, 3072\n","embeddings"],"metadata":{"id":"96nLFF1ja0k_","executionInfo":{"status":"ok","timestamp":1725218110312,"user_tz":-180,"elapsed":1565,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"ab1ad8fb-1fa4-40ce-ac6d-7cfac2ef68dc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7bd12c1e5d80>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7bd12c20af50>, model='text-embedding-3-large', dimensions=3072, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","index=Chroma.from_documents(documents=pdf_doc,\n","                            embedding=embeddings,\n","                            persist_directory=\"/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/vectorstore\") # persist_directory, saves in the directory\n","\n","retriever=index.as_retriever()"],"metadata":{"id":"VqZ7XBwoa0ee","executionInfo":{"status":"ok","timestamp":1725219189859,"user_tz":-180,"elapsed":2346,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["retriever=index.as_retriever(search_kwargs={\"k\": 5})"],"metadata":{"id":"l8m5bfwMzdO8","executionInfo":{"status":"ok","timestamp":1725219193015,"user_tz":-180,"elapsed":290,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["retriever.invoke(\"What is BERT?\")"],"metadata":{"id":"ZG2PzaHk0SfR","executionInfo":{"status":"ok","timestamp":1725219196222,"user_tz":-180,"elapsed":804,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"3d13e734-2031-4212-f231-114d6c8510cc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n"," Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked'),\n"," Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used'),\n"," Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='A distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,'),\n"," Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and')]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["### Load Vectorstore(index) From Your Drive"],"metadata":{"id":"y2tMqUthPchD"}},{"cell_type":"code","source":["loaded_index=Chroma(persist_directory=\"/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/vectorstore\",\n","                    embedding_function=embeddings)"],"metadata":{"id":"5pjKXmO6a3En","executionInfo":{"status":"ok","timestamp":1725219201126,"user_tz":-180,"elapsed":338,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"],"metadata":{"id":"pKA0PgNJQOmj"}},{"cell_type":"code","source":["def retrieve_query(query,k=5):\n","    retriever=index.as_retriever(search_kwargs={\"k\": k}) #loaded_index\n","    return retriever.invoke(query)\n"],"metadata":{"id":"KRH-FWEua5Fn","executionInfo":{"status":"ok","timestamp":1725219232264,"user_tz":-180,"elapsed":367,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["our_query = \"What is fine-tuning?\"\n","\n","doc_search=retrieve_query(our_query, k=5) # first two most similar texts are returned\n","doc_search"],"metadata":{"id":"80TWdFk6a4-3","executionInfo":{"status":"ok","timestamp":1725219280475,"user_tz":-180,"elapsed":1520,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"8d943a6f-1322-418f-892e-2cc6ac9cd42c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='answering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R'),\n"," Document(metadata={'page': 13, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='[SEP], [CLS] and sentence A/B embed\\x02dings during pre-training.\\r\\n• GPT was trained for 1M steps with a batch\\r\\nsize of 32,000 words; BERT was trained for\\r\\n1M steps with a batch size of 128,000 words.\\r\\n• GPT used the same learning rate of 5e-5 for\\r\\nall fine-tuning experiments; BERT chooses a\\r\\ntask-specific fine-tuning learning rate which\\r\\nperforms the best on the development set.\\r\\nTo isolate the effect of these differences, we per\\x02form ablation experiments in Section 5.1 which\\r\\ndemonstrate that the majority of the improvements\\r\\nare in fact coming from the two pre-training tasks\\r\\nand the bidirectionality they enable.\\r\\nA.5 Illustrations of Fine-tuning on Different\\r\\nTasks\\r\\nThe illustration of fine-tuning BERT on different\\r\\ntasks can be seen in Figure 4. Our task-specific\\r\\nmodels are formed by incorporating BERT with\\r\\none additional output layer, so a minimal num\\x02ber of parameters need to be learned from scratch.\\r\\nAmong the tasks, (a) and (b) are sequence-level'),\n"," Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='included in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.'),\n"," Document(metadata={'page': 12, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='positional embeddings.\\r\\nA.3 Fine-tuning Procedure\\r\\nFor fine-tuning, most model hyperparameters are\\r\\nthe same as in pre-training, with the exception of\\r\\nthe batch size, learning rate, and number of train\\x02ing epochs. The dropout probability was always\\r\\nkept at 0.1. The optimal hyperparameter values\\r\\nare task-specific, but we found the following range\\r\\nof possible values to work well across all tasks:\\r\\n• Batch size: 16, 32\\r\\n13https://cloudplatform.googleblog.com/2018/06/Cloud\\x02TPU-now-offers-preemptible-pricing-and-global\\x02availability.html'),\n"," Document(metadata={'page': 4, 'source': '/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf'}, page_content='model pre-training. For the pre-training corpus we\\r\\nuse the BooksCorpus (800M words) (Zhu et al.,\\r\\n2015) and English Wikipedia (2,500M words).\\r\\nFor Wikipedia we extract only the text passages\\r\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\r\\nshuffled sentence-level corpus such as the Billion\\r\\nWord Benchmark (Chelba et al., 2013) in order to\\r\\nextract long contiguous sequences.\\r\\n3.2 Fine-tuning BERT\\r\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\r\\nwhether they involve single text or text pairs—by\\r\\nswapping out the appropriate inputs and outputs.\\r\\nFor applications involving text pairs, a common\\r\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\r\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["### Generating an Answer Based on The Similar Chunks"],"metadata":{"id":"-G8R4V7BROkz"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","----------------\n","{context}\"\"\"\n","\n","prompt_template = PromptTemplate(\n","    input_variables =['question','context'],\n","    template = template\n",")"],"metadata":{"id":"XNDU0jcma7HB","executionInfo":{"status":"ok","timestamp":1725219385052,"user_tz":-180,"elapsed":308,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n","               temperature=0,\n","               top_p=1)\n","\n","chain = prompt_template | llm | StrOutputParser()\n","\n","output= chain.invoke({\"question\":our_query, \"context\":doc_search}) # first four most similar texts are returned\n","output"],"metadata":{"id":"aUnTIxSoa6_y","executionInfo":{"status":"ok","timestamp":1725219388215,"user_tz":-180,"elapsed":2108,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"37ee946f-ccf8-425c-bc14-41eb1a159137","colab":{"base_uri":"https://localhost:8080/","height":86}},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. It is relatively inexpensive compared to pre-training and typically involves adjusting a few parameters, such as the classification layer weights, while keeping most of the model's hyperparameters the same as in pre-training. Fine-tuning allows the model to adapt to the nuances of the new task while leveraging the knowledge it gained during pre-training.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["from IPython.display import Markdown\n","\n","Markdown(output)"],"metadata":{"id":"7nYtGHiM1Qi0","executionInfo":{"status":"ok","timestamp":1725219389864,"user_tz":-180,"elapsed":361,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"d2514379-e834-4c52-ed65-64eb5b38b4c9","colab":{"base_uri":"https://localhost:8080/","height":97}},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. It is relatively inexpensive compared to pre-training and typically involves adjusting a few parameters, such as the classification layer weights, while keeping most of the model's hyperparameters the same as in pre-training. Fine-tuning allows the model to adapt to the nuances of the new task while leveraging the knowledge it gained during pre-training."},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### Pipeline For RAG (If you want, you can use the gemini-1.5-pro model)"],"metadata":{"id":"sy4fmzsWLayT"}},{"cell_type":"code","source":["def get_answers(query):\n","    from langchain_openai import ChatOpenAI\n","    from langchain_core.output_parsers import StrOutputParser\n","    from langchain.prompts import PromptTemplate\n","    from IPython.display import Markdown\n","\n","    doc_search=retrieve_query(query) # most similar texts are returned\n","\n","\n","    template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n","    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","    ----------------\n","    {context}\"\"\"\n","\n","    prompt_template = PromptTemplate(\n","    input_variables =['question','context'],\n","    template = template)\n","\n","\n","    llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n","                  temperature=0,\n","                  top_p=1)\n","\n","    chain = prompt_template | llm | StrOutputParser()\n","\n","    output= chain.invoke({\"question\":query, \"context\":doc_search}) # first four most similar texts are returned\n","    return Markdown(output)"],"metadata":{"id":"_aSt7YgIa9jo","executionInfo":{"status":"ok","timestamp":1725219395542,"user_tz":-180,"elapsed":304,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["our_query = \"What is BERT?\"\n","answer = get_answers(our_query)\n","answer"],"metadata":{"id":"2PzfTncDa9dO","executionInfo":{"status":"ok","timestamp":1725219403062,"user_tz":-180,"elapsed":2443,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"9bcff70d-8941-4c1f-e99b-74ad5aa3c19d","colab":{"base_uri":"https://localhost:8080/","height":114}},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation model introduced by Google AI Language. It is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. This allows BERT to achieve state-of-the-art results on various natural language processing tasks, such as question answering and language inference, without requiring substantial task-specific architecture modifications. BERT uses a \"masked language model\" pre-training objective, which involves randomly masking some tokens in the input and predicting the original tokens. Its architecture is based on a multi-layer bidirectional Transformer encoder."},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["our_query = \"What is fine-tuning?\"\n","answer = get_answers(our_query)\n","answer"],"metadata":{"id":"MsOzafDs8DDV","executionInfo":{"status":"ok","timestamp":1725219407189,"user_tz":-180,"elapsed":1887,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"40a781ad-4957-46b5-c729-f5ed1a7b0bf2","colab":{"base_uri":"https://localhost:8080/","height":97}},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset. This involves adjusting the model's parameters to improve its performance on that particular task, often by adding a task-specific output layer. Fine-tuning is generally less resource-intensive than the initial pre-training phase and can be completed relatively quickly, allowing the model to adapt to new tasks while leveraging the knowledge it gained during pre-training."},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## Project 2: Generating PDF Document Summaries\n","\n","In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n","\n","### **Project Steps:**\n","- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n","\n","- **2.Summarization Techniques:**\n","\n","  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n","\n","  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n","\n","  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n","\n","### Important Notes:\n","\n","- Models like GPT-4 and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n","\n","- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n","Best of luck!"],"metadata":{"id":"H9GmKlL2NRff"}},{"cell_type":"markdown","source":["### Install Libraries"],"metadata":{"id":"WhjLe0IqRnl4"}},{"cell_type":"code","source":["!pip install -qU langchain-openai"],"metadata":{"id":"ZXdV8CcqbFrW","executionInfo":{"status":"ok","timestamp":1725280816371,"user_tz":-180,"elapsed":10169,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"1bb9de4f-ec3f-4f0d-988b-85ff176a7c3a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU langchain-community"],"metadata":{"id":"rcFsXQwCbFkm","executionInfo":{"status":"ok","timestamp":1725280830049,"user_tz":-180,"elapsed":13685,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"e63ee0ee-2aa5-4763-d085-ea6d1045b9fb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU pypdfium2"],"metadata":{"id":"W9H-cdpMihdD","executionInfo":{"status":"ok","timestamp":1725280834487,"user_tz":-180,"elapsed":4443,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"56024bce-f220-4d18-b045-7851990e2cf4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m526.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Loading PDF Document"],"metadata":{"id":"yqImlx_IRqQS"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFium2Loader\n","\n","def read_doc(directory):\n","    file_loader=PyPDFium2Loader(directory)\n","    pdf_documents=file_loader.load()\n","    return pdf_documents"],"metadata":{"id":"CCkT3msfbH_n","executionInfo":{"status":"ok","timestamp":1725280839114,"user_tz":-180,"elapsed":1330,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["pdf=read_doc('/content/drive/MyDrive/Colab Notebooks/Capstone_NLP/N19-1423.pdf')\n","len(pdf)"],"metadata":{"id":"5a_FpBOcbHzP","executionInfo":{"status":"ok","timestamp":1725280930346,"user_tz":-180,"elapsed":1938,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"1036a00e-af33-4183-eb44-d41e9c43834a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n","  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"]},{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"],"metadata":{"id":"LuyT0IoWR4n8"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.chains.summarize import load_summarize_chain\n","\n","llm = ChatOpenAI(temperature=0,\n","                 model_name='gpt-4o-mini',\n","                 max_tokens=1024)"],"metadata":{"id":"O3yAnW3PbKIX","executionInfo":{"status":"ok","timestamp":1725280981735,"user_tz":-180,"elapsed":440,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["chain = load_summarize_chain(\n","    llm,\n","    chain_type='stuff'\n",")\n","output_summary = chain.invoke(pdf[0:5])['output_text']"],"metadata":{"id":"8wopgGPibKA3","executionInfo":{"status":"ok","timestamp":1725280987941,"user_tz":-180,"elapsed":3327,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown\n","Markdown(output_summary)"],"metadata":{"id":"y0VBhsTxi4qf","executionInfo":{"status":"ok","timestamp":1725280989695,"user_tz":-180,"elapsed":5,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"d5b5ce28-da43-4f9b-ccef-84cc1be57e4e","colab":{"base_uri":"https://localhost:8080/","height":131}},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by jointly considering both left and right contexts, overcoming limitations of previous unidirectional models. It employs a masked language model (MLM) and a next sentence prediction (NSP) task during pre-training, allowing it to achieve state-of-the-art results on eleven natural language processing tasks, including question answering and language inference. BERT's architecture is unified across tasks, requiring minimal task-specific modifications during fine-tuning. The model demonstrates significant improvements over existing approaches, achieving notable performance metrics on benchmarks like GLUE and SQuAD. The code and pre-trained models are publicly available for further research and application."},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["### Document Splitter"],"metadata":{"id":"JvrLsoivTulb"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","\n","\n","def chunk_data(docs, chunk_size=10000, chunk_overlap=200):\n","    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n","                                                 chunk_overlap=chunk_overlap)\n","    chunks=text_splitter.split_documents(docs)\n","    return chunks"],"metadata":{"id":"5j9NMbSCbMyf","executionInfo":{"status":"ok","timestamp":1725285213953,"user_tz":-180,"elapsed":419,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["chunks=chunk_data(docs=pdf)"],"metadata":{"id":"53mpwb7KbMrf","executionInfo":{"status":"ok","timestamp":1725285216639,"user_tz":-180,"elapsed":589,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["len(chunks)"],"metadata":{"id":"RPU26AbF0aGq","executionInfo":{"status":"ok","timestamp":1725285218036,"user_tz":-180,"elapsed":10,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"9989afd9-f4ad-48ca-9c05-9d7dcc49085b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\" and \"refine\""],"metadata":{"id":"3zlVe2iISX0Q"}},{"cell_type":"markdown","source":["map_reduce\n","\n","---\n","\n"],"metadata":{"id":"W8PaW7R8pGOa"}},{"cell_type":"code","source":["\n","llm = ChatOpenAI(temperature=0,\n","                 model_name='gpt-4o-mini',\n","                 max_tokens=1024)"],"metadata":{"id":"t6wPW3OFbOcJ","executionInfo":{"status":"ok","timestamp":1725285222443,"user_tz":-180,"elapsed":852,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["chain = load_summarize_chain(llm,\n","                             chain_type=\"map_reduce\")\n","\n","\n","output_summary = chain.invoke(chunks)[\"output_text\"]\n","Markdown(output_summary)"],"metadata":{"id":"8NpJYGxRbOVC","executionInfo":{"status":"ok","timestamp":1725285529047,"user_tz":-180,"elapsed":306052,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"3d4b9005-eaf9-4d3f-c001-7373f811413a","colab":{"base_uri":"https://localhost:8080/","height":262}},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model that pre-trains deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers. BERT can be fine-tuned with an additional output layer to achieve state-of-the-art results across various NLP tasks, such as question answering and language inference, without significant task-specific modifications. BERT's effectiveness is demonstrated by achieving new benchmarks on eleven NLP tasks, including substantial improvements in GLUE score, MultiNLI accuracy, and SQuAD question answering metrics. The model employs a masked language model pre-training objective, enhancing its ability to incorporate context from both directions.\n\nBERT's architecture is a multi-layer bidirectional Transformer encoder, which allows it to consider context from both directions. It uses special tokens ([CLS] and [SEP]) and WordPiece embeddings with a 30,000 token vocabulary. Pre-training involves masked language modeling and next sentence prediction tasks, while fine-tuning adapts the pre-trained model to specific tasks using labeled data. BERT comes in two sizes, BERTBASE and BERTLARGE, differing in the number of layers, hidden size, and attention heads.\n\nThe paper highlights BERT's superior performance on various benchmarks, including GLUE, SQuAD, and SWAG, significantly outperforming previous models like OpenAI GPT and ELMo. The study underscores the importance of deep bidirectionality and sufficient pre-training for achieving high performance in NLP tasks. BERT's code and pre-trained models are available online, facilitating further research and application in the field."},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["refine\n","\n","---\n","\n"],"metadata":{"id":"w-RFYVgupQ0W"}},{"cell_type":"code","source":["chain = load_summarize_chain(llm,\n","                             chain_type=\"refine\")\n","\n","output_summary = chain.invoke(chunks)[\"output_text\"]\n","Markdown(output_summary)"],"metadata":{"id":"xdrbZ73gpVIs","executionInfo":{"status":"ok","timestamp":1725285954671,"user_tz":-180,"elapsed":419223,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"354bc65c-8ed2-485b-d9f3-188b37a913c1","colab":{"base_uri":"https://localhost:8080/","height":643}},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers. Unlike previous models, BERT can be fine-tuned with minimal task-specific modifications to achieve state-of-the-art results across various natural language processing tasks, including question answering and language inference. BERT's effectiveness is demonstrated by its superior performance on eleven NLP tasks, significantly improving benchmarks like the GLUE score, MultiNLI accuracy, and SQuAD question answering tests. The model addresses limitations of prior unidirectional approaches by employing a masked language model (MLM) pre-training objective, enhancing its ability to incorporate context from both directions. Additionally, BERT uses a \"next sentence prediction\" task to jointly pre-train text-pair representations, further improving its performance. The paper highlights the importance of bidirectional pre-training and shows that BERT reduces the need for heavily-engineered task-specific architectures, setting new standards in NLP. The code and pre-trained models are available at https://github.com/google-research/bert.\n\nBERT's architecture is a multi-layer bidirectional Transformer encoder, based on the original implementation described by Vaswani et al. (2017). The model comes in two sizes: BERTBASE with 12 layers, 768 hidden units, and 12 self-attention heads, totaling 110 million parameters, and BERTLARGE with 24 layers, 1024 hidden units, and 16 self-attention heads, totaling 340 million parameters. The bidirectional self-attention mechanism in BERT allows it to attend to both left and right context, unlike the unidirectional self-attention used in models like OpenAI's GPT. This unified architecture across different tasks minimizes the differences between the pre-trained and fine-tuned models, making BERT highly versatile and effective for a wide range of NLP applications.\n\nTo handle a variety of downstream tasks, BERT's input representation can unambiguously represent both single sentences and pairs of sentences in one token sequence. The model uses WordPiece embeddings with a 30,000 token vocabulary. Each sequence starts with a special classification token ([CLS]), and sentence pairs are separated by a special token ([SEP]). A learned embedding is added to each token to indicate sentence membership. BERT is pre-trained using two unsupervised tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). MLM involves masking a percentage of input tokens at random and predicting them, while NSP involves predicting whether one sentence follows another in a text. These tasks enable BERT to understand context and relationships between sentences, enhancing its performance on tasks like QA and NLI.\n\nThe pre-training data for BERT includes the BooksCorpus (800M words) and English Wikipedia (2,500M words), focusing on document-level corpora to extract long contiguous sequences. Fine-tuning BERT is straightforward due to the self-attention mechanism, which allows the model to handle various downstream tasks by simply swapping out the appropriate inputs and outputs. For text pair tasks, BERT uses self-attention to encode concatenated text pairs, effectively including bidirectional cross attention. Fine-tuning is relatively inexpensive, with all results replicable in at most 1 hour on a single Cloud TPU or a few hours on a GPU. The paper presents fine-tuning results on 11 NLP tasks, including the GLUE benchmark, demonstrating BERT's superior performance.\n\nBERTBASE and BERTLARGE outperform previous models on all GLUE tasks by a substantial margin, with BERTLARGE achieving an average accuracy improvement of 7.0% over the prior state of the art. BERT also excels in the SQuAD v1.1 question answering task, outperforming top leaderboard systems. Fine-tuning BERT on small datasets can be unstable, but using random restarts and data shuffling helps achieve the best performance. Overall, BERT sets new benchmarks in NLP, demonstrating the effectiveness of bidirectional pre-training and the versatility of its architecture.\n\nBERT's performance on the SQuAD 1.1 and SQuAD 2.0 datasets further underscores its capabilities. In SQuAD 1.1, BERTLARGE (Single) achieves an F1 score of 90.9, while BERTLARGE (Ensemble) reaches 91.8, surpassing previous top systems. For SQuAD 2.0, BERTLARGE (Single) achieves an F1 score of 83.1, outperforming the previous best system by 5.1 F1 points. Additionally, BERTLARGE outperforms other models on the SWAG dataset, achieving an accuracy of 86.3 on the test set, which is 8.3% higher than OpenAI GPT. These results highlight BERT's robustness and adaptability across different NLP tasks.\n\nAblation studies reveal the importance of BERT's pre-training tasks. Removing the Next Sentence Prediction task"},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."],"metadata":{"id":"9zZxse-ZUV3S"}},{"cell_type":"code","source":["chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type='map_reduce'\n",")\n","chain"],"metadata":{"id":"lpuiEedJbQME","executionInfo":{"status":"ok","timestamp":1725285994349,"user_tz":-180,"elapsed":436,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"e45c3418-088b-488c-f949-a244a5119176","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b079c23b520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b079c273550>, root_client=<openai.OpenAI object at 0x7b079c23a620>, root_async_client=<openai.AsyncOpenAI object at 0x7b079c2397e0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b079c23b520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b079c273550>, root_client=<openai.OpenAI object at 0x7b079c23a620>, root_async_client=<openai.AsyncOpenAI object at 0x7b079c2397e0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","chunks_prompt=\"\"\"\n","Please summarize the below text:\n","text:'{text}'\n","summary:\n","\"\"\"\n","map_prompt_template=PromptTemplate(input_variables=['text'],\n","                                   template=chunks_prompt)"],"metadata":{"id":"03H0WLJ9EJTl","executionInfo":{"status":"ok","timestamp":1725286174679,"user_tz":-180,"elapsed":441,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","final_combine_prompt=\"\"\"\n","Provide a final summary of the entire text with at least 1000 Tokens with important points.\n","Add a Generic  Title,\n","Start the precise summary with an introduction and provide Key Points Using Bullet Points\n","text: '{text}'\n","summary:\n","\"\"\"\n","final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n","                                             template=final_combine_prompt)"],"metadata":{"id":"OdFF9bOAbQEo","executionInfo":{"status":"ok","timestamp":1725286175942,"user_tz":-180,"elapsed":5,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["chain = load_summarize_chain(\n","                            llm=llm,\n","                            chain_type='map_reduce',\n","                            map_prompt=map_prompt_template,\n","                            combine_prompt=final_combine_prompt_template\n",")\n","chain"],"metadata":{"id":"afvkFsIwr0De","executionInfo":{"status":"ok","timestamp":1725286178120,"user_tz":-180,"elapsed":430,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"cbce431c-4901-4dbd-b23e-ff4a1989d649","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nPlease summarize the below text:\\ntext:'{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b079c23b520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b079c273550>, root_client=<openai.OpenAI object at 0x7b079c23a620>, root_async_client=<openai.AsyncOpenAI object at 0x7b079c2397e0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template=\"\\nProvide a final summary of the entire text with at least 1000 Tokens with important points.\\nAdd a Generic  Title,\\nStart the precise summary with an introduction and provide Key Points Using Bullet Points\\ntext: '{text}'\\nsummary:\\n\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b079c23b520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b079c273550>, root_client=<openai.OpenAI object at 0x7b079c23a620>, root_async_client=<openai.AsyncOpenAI object at 0x7b079c2397e0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["output_summary = chain.invoke(chunks)[\"output_text\"]\n","output_summary"],"metadata":{"id":"TTNiJZckt1Ea","executionInfo":{"status":"ok","timestamp":1725286533074,"user_tz":-180,"elapsed":352827,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"13022ed3-c875-4d7a-d6a2-71367a3c7525","colab":{"base_uri":"https://localhost:8080/","height":154}},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"# Comprehensive Overview of BERT: Revolutionizing Language Representation Models\\n\\n## Introduction\\nBERT (Bidirectional Encoder Representations from Transformers) is a transformative language representation model developed by Google AI Language. It has set new benchmarks in natural language processing (NLP) by leveraging bidirectional context in all layers, allowing it to achieve state-of-the-art results across various NLP tasks. This summary provides an in-depth overview of BERT's architecture, pre-training and fine-tuning processes, performance on benchmarks, and comparisons with other models. Additionally, it delves into the intricacies of BERT's pre-training process, examining the effects of the number of training steps and various masking procedures.\\n\\n## Key Points\\n\\n### BERT's Innovative Approach\\n- **Bidirectional Context**: BERT captures context from both directions using a masked language model (MLM) objective, unlike previous models that used unidirectional or shallow bidirectional methods.\\n- **Next Sentence Prediction (NSP)**: BERT uses NSP to pre-train text-pair representations, enhancing its ability to understand sentence relationships and context.\\n- **Unified Architecture**: BERT employs a multi-layer bidirectional Transformer encoder, pre-trained on unlabeled data and fine-tuned on labeled data for various downstream tasks.\\n\\n### Pre-Training and Fine-Tuning\\n- **Input Representations**: BERT can represent both single sentences and pairs of sentences in one token sequence using WordPiece embeddings with a 30,000 token vocabulary. Special tokens ([CLS] and [SEP]) are used for classification and sentence separation.\\n- **Pre-Training Tasks**: \\n  - **Masked Language Modeling (MLM)**: Random tokens are masked and predicted, allowing for bidirectional context understanding.\\n  - **Next Sentence Prediction (NSP)**: The model predicts if one sentence follows another, aiding tasks like Question Answering and Natural Language Inference.\\n- **Fine-Tuning**: BERT is fine-tuned by adjusting inputs and outputs for specific tasks, with minimal differences between pre-trained and final downstream architectures. Fine-tuning is relatively quick, taking about an hour on a Cloud TPU.\\n\\n### Performance on Benchmarks\\n- **GLUE Benchmark**:\\n  - BERTBASE and BERTLARGE outperform previous state-of-the-art models across all tasks.\\n  - BERTLARGE shows significant improvements, especially on tasks with limited training data, achieving an average accuracy improvement of 7.0% over prior state-of-the-art models.\\n  - On the MNLI task, BERT achieves a 4.6% absolute accuracy improvement.\\n  - BERTLARGE scores 80.5 on the official GLUE leaderboard, compared to OpenAI GPT's 72.8.\\n- **SQuAD v1.1**:\\n  - BERT models are fine-tuned using a specific method involving start and end vectors for answer span prediction.\\n  - BERT outperforms top leaderboard systems, achieving +1.5 F1 in ensembling and +1.3 F1 as a single system.\\n  - The single BERT model surpasses the top ensemble system in F1 score.\\n- **SQuAD 2.0 and SWAG**:\\n  - BERTLARGE (Single) achieved 78.7 EM and 81.9 F1 on the Dev set, and 80.0 EM and 83.1 F1 on the Test set, outperforming other systems that do not use BERT.\\n  - BERTLARGE achieved 86.6 accuracy on the Dev set and 86.3 on the Test set for SWAG, outperforming ESIM+ELMo and OpenAI GPT by significant margins.\\n\\n### Impact of Pre-Training Tasks and Model Size\\n- **Pre-Training Tasks**:\\n  - Removing the NSP task slightly reduces performance on QNLI, MNLI, and SQuAD tasks.\\n  - Training a left-to-right (LTR) language model without NSP significantly degrades performance, especially on MRPC and SQuAD tasks.\\n  - The importance of bidirectional context is highlighted for tasks like QA, where token predictions benefit from both left and right context.\\n- **Model Size**:\\n  - Larger BERT models consistently show better accuracy across various tasks, even with small datasets like MRPC.\\n  - BERTBASE has 110M parameters and BERTLARGE has 340M parameters, with larger models leading to improvements in both large-scale and small-scale tasks.\\n\\n### Comparisons with Other Models\\n- **BERT vs. OpenAI GPT and ELMo**:\\n  - BERT uses a bidirectional Transformer, OpenAI GPT uses a left-to-right Transformer, and ELMo uses concatenated left-to-right and right-to-left LSTMs.\\n  - BERT and OpenAI GPT are fine-tuning approaches, while ELMo is feature-based.\\n  - BERT's improvements are attributed to its bidirectionality and pre-training tasks.\\n\\n### Fine-Tuning and Hyperparameters\\n- **Hyperparameters\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["from IPython.display import Markdown\n","\n","Markdown(output_summary)"],"metadata":{"id":"uiOPF1Brt-dU","executionInfo":{"status":"ok","timestamp":1725286599226,"user_tz":-180,"elapsed":455,"user":{"displayName":"Emre Demirok","userId":"12505043739975564470"}},"outputId":"cdcfb1d2-9fe7-4d9a-c2ab-725503b9fd92","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"# Comprehensive Overview of BERT: Revolutionizing Language Representation Models\n\n## Introduction\nBERT (Bidirectional Encoder Representations from Transformers) is a transformative language representation model developed by Google AI Language. It has set new benchmarks in natural language processing (NLP) by leveraging bidirectional context in all layers, allowing it to achieve state-of-the-art results across various NLP tasks. This summary provides an in-depth overview of BERT's architecture, pre-training and fine-tuning processes, performance on benchmarks, and comparisons with other models. Additionally, it delves into the intricacies of BERT's pre-training process, examining the effects of the number of training steps and various masking procedures.\n\n## Key Points\n\n### BERT's Innovative Approach\n- **Bidirectional Context**: BERT captures context from both directions using a masked language model (MLM) objective, unlike previous models that used unidirectional or shallow bidirectional methods.\n- **Next Sentence Prediction (NSP)**: BERT uses NSP to pre-train text-pair representations, enhancing its ability to understand sentence relationships and context.\n- **Unified Architecture**: BERT employs a multi-layer bidirectional Transformer encoder, pre-trained on unlabeled data and fine-tuned on labeled data for various downstream tasks.\n\n### Pre-Training and Fine-Tuning\n- **Input Representations**: BERT can represent both single sentences and pairs of sentences in one token sequence using WordPiece embeddings with a 30,000 token vocabulary. Special tokens ([CLS] and [SEP]) are used for classification and sentence separation.\n- **Pre-Training Tasks**: \n  - **Masked Language Modeling (MLM)**: Random tokens are masked and predicted, allowing for bidirectional context understanding.\n  - **Next Sentence Prediction (NSP)**: The model predicts if one sentence follows another, aiding tasks like Question Answering and Natural Language Inference.\n- **Fine-Tuning**: BERT is fine-tuned by adjusting inputs and outputs for specific tasks, with minimal differences between pre-trained and final downstream architectures. Fine-tuning is relatively quick, taking about an hour on a Cloud TPU.\n\n### Performance on Benchmarks\n- **GLUE Benchmark**:\n  - BERTBASE and BERTLARGE outperform previous state-of-the-art models across all tasks.\n  - BERTLARGE shows significant improvements, especially on tasks with limited training data, achieving an average accuracy improvement of 7.0% over prior state-of-the-art models.\n  - On the MNLI task, BERT achieves a 4.6% absolute accuracy improvement.\n  - BERTLARGE scores 80.5 on the official GLUE leaderboard, compared to OpenAI GPT's 72.8.\n- **SQuAD v1.1**:\n  - BERT models are fine-tuned using a specific method involving start and end vectors for answer span prediction.\n  - BERT outperforms top leaderboard systems, achieving +1.5 F1 in ensembling and +1.3 F1 as a single system.\n  - The single BERT model surpasses the top ensemble system in F1 score.\n- **SQuAD 2.0 and SWAG**:\n  - BERTLARGE (Single) achieved 78.7 EM and 81.9 F1 on the Dev set, and 80.0 EM and 83.1 F1 on the Test set, outperforming other systems that do not use BERT.\n  - BERTLARGE achieved 86.6 accuracy on the Dev set and 86.3 on the Test set for SWAG, outperforming ESIM+ELMo and OpenAI GPT by significant margins.\n\n### Impact of Pre-Training Tasks and Model Size\n- **Pre-Training Tasks**:\n  - Removing the NSP task slightly reduces performance on QNLI, MNLI, and SQuAD tasks.\n  - Training a left-to-right (LTR) language model without NSP significantly degrades performance, especially on MRPC and SQuAD tasks.\n  - The importance of bidirectional context is highlighted for tasks like QA, where token predictions benefit from both left and right context.\n- **Model Size**:\n  - Larger BERT models consistently show better accuracy across various tasks, even with small datasets like MRPC.\n  - BERTBASE has 110M parameters and BERTLARGE has 340M parameters, with larger models leading to improvements in both large-scale and small-scale tasks.\n\n### Comparisons with Other Models\n- **BERT vs. OpenAI GPT and ELMo**:\n  - BERT uses a bidirectional Transformer, OpenAI GPT uses a left-to-right Transformer, and ELMo uses concatenated left-to-right and right-to-left LSTMs.\n  - BERT and OpenAI GPT are fine-tuning approaches, while ELMo is feature-based.\n  - BERT's improvements are attributed to its bidirectionality and pre-training tasks.\n\n### Fine-Tuning and Hyperparameters\n- **Hyperparameters"},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["# GPT modelini döküman dışına çıkmasın diye (döküman içerisinde geçen bilgi dışına) agent tool kullanılmadı."],"metadata":{"id":"NSHnBPURvFln"}},{"cell_type":"markdown","source":["___\n","\n","<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n","\n","___"],"metadata":{"id":"FLJmnz9TVCRL"}}]}